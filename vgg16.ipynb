{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Blood Cell Subtype Classification Part 2: Transfer Learning using a finetuned Densenet201 model. \n\nFor Part 1 please follow [this link](https://www.kaggle.com/kbrans/cnn-91-6-acc-with-new-train-val-test-splits). A lot of the code from Part 1 has been repeated in this notebook, and the explanations and commentary for some design decisions can be found there (e.g new Train/Val/Test splits). \n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport seaborn as sns\nfrom tqdm import tqdm \nfrom sklearn.utils import shuffle\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport tensorflow as tf\nimport keras\nfrom keras.applications.vgg16 import VGG16 \nfrom keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Sequential, Model \nfrom keras.applications import DenseNet201\nfrom keras.applications import VGG16\nfrom keras.initializers import he_normal\nfrom keras.layers import Lambda, SeparableConv2D, BatchNormalization, Dropout, MaxPooling2D, Input, Dense, Conv2D, Activation, Flatten \nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2022-11-22T09:48:04.074614Z","iopub.execute_input":"2022-11-22T09:48:04.074970Z","iopub.status.idle":"2022-11-22T09:48:04.083040Z","shell.execute_reply.started":"2022-11-22T09:48:04.074935Z","shell.execute_reply":"2022-11-22T09:48:04.081936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading data","metadata":{}},{"cell_type":"code","source":"class_names = ['EOSINOPHIL', 'LYMPHOCYTE', 'MONOCYTE', 'NEUTROPHIL']\nnb_classes = len(class_names)\nimage_size = (150,150)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:48:07.884846Z","iopub.execute_input":"2022-11-22T09:48:07.885238Z","iopub.status.idle":"2022-11-22T09:48:07.889717Z","shell.execute_reply.started":"2022-11-22T09:48:07.885204Z","shell.execute_reply":"2022-11-22T09:48:07.888760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_data():\n\n    datasets = ['/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TRAIN','/kaggle/input/blood-cells/dataset2-master/dataset2-master/images/TEST' ]\n    images = []\n    labels = []\n\n    # iterate through training and test sets\n    for dataset in datasets:\n\n        # iterate through folders in each dataset\n        for folder in os.listdir(dataset):\n\n            if folder in ['EOSINOPHIL']: label = 0\n            elif folder in ['LYMPHOCYTE']: label = 1\n            elif folder in ['MONOCYTE']: label = 2\n            elif folder in ['NEUTROPHIL']: label = 3\n\n            # iterate through each image in folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n\n                # get pathname of each image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n\n                # Open and resize the| img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, image_size)\n\n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n\n    images = np.array(images, dtype = 'float32')\n    labels = np.array(labels, dtype = 'int32')\n\n    return images, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:48:36.689573Z","iopub.execute_input":"2022-11-22T09:48:36.690041Z","iopub.status.idle":"2022-11-22T09:48:36.699979Z","shell.execute_reply.started":"2022-11-22T09:48:36.689988Z","shell.execute_reply":"2022-11-22T09:48:36.699051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels = load_data()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:48:41.831212Z","iopub.execute_input":"2022-11-22T09:48:41.831558Z","iopub.status.idle":"2022-11-22T09:49:50.734766Z","shell.execute_reply.started":"2022-11-22T09:48:41.831526Z","shell.execute_reply":"2022-11-22T09:49:50.733784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"images, labels = shuffle(images, labels, random_state=10)\n\ntrain_images, test_images, train_labels, test_labels = train_test_split(images, labels, test_size = 0.2)\ntest_images, val_images, test_labels, val_labels = train_test_split(test_images, test_labels, test_size = 0.5)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:02.255885Z","iopub.execute_input":"2022-11-22T09:50:02.256267Z","iopub.status.idle":"2022-11-22T09:50:04.477524Z","shell.execute_reply.started":"2022-11-22T09:50:02.256234Z","shell.execute_reply":"2022-11-22T09:50:04.476589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration","metadata":{}},{"cell_type":"code","source":"n_train = train_labels.shape[0]\nn_val = val_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint(\"Number of training examples: {}\".format(n_train))\nprint(\"Number of validation examples: {}\".format(n_val))\nprint(\"Number of testing examples: {}\".format(n_test))\n\nprint(\"Training images are of shape: {}\".format(train_images.shape))\nprint(\"Training labels are of shape: {}\".format(train_labels.shape))\nprint(\"Validation images are of shape: {}\".format(val_images.shape))\nprint(\"Validation labels are of shape: {}\".format(val_labels.shape))\nprint(\"Test images are of shape: {}\".format(test_images.shape))\nprint(\"Test labels are of shape: {}\".format(test_labels.shape))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:08.456249Z","iopub.execute_input":"2022-11-22T09:50:08.456622Z","iopub.status.idle":"2022-11-22T09:50:08.467079Z","shell.execute_reply.started":"2022-11-22T09:50:08.456586Z","shell.execute_reply":"2022-11-22T09:50:08.465992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, train_counts = np.unique(train_labels, return_counts = True)\n_, val_counts = np.unique(val_labels, return_counts = True)\n_, test_counts = np.unique(test_labels, return_counts = True)\n\npd.DataFrame({'train': train_counts, \"val\": val_counts, \"test\": test_counts}, index = class_names).plot.bar()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:11.293597Z","iopub.execute_input":"2022-11-22T09:50:11.293952Z","iopub.status.idle":"2022-11-22T09:50:11.495132Z","shell.execute_reply.started":"2022-11-22T09:50:11.293904Z","shell.execute_reply":"2022-11-22T09:50:11.494042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.pie(train_counts,\n        explode=(0, 0, 0, 0) , \n        labels=class_names,\n        autopct='%1.1f%%')\nplt.axis('equal')\nplt.title('Proportion of each observed category')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:13.125838Z","iopub.execute_input":"2022-11-22T09:50:13.126213Z","iopub.status.idle":"2022-11-22T09:50:13.187771Z","shell.execute_reply.started":"2022-11-22T09:50:13.126180Z","shell.execute_reply":"2022-11-22T09:50:13.186869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = train_images / 255.0 \nval_images = val_images / 255.0\ntest_images = test_images / 255.0","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:14.702743Z","iopub.execute_input":"2022-11-22T09:50:14.703115Z","iopub.status.idle":"2022-11-22T09:50:15.688361Z","shell.execute_reply.started":"2022-11-22T09:50:14.703078Z","shell.execute_reply":"2022-11-22T09:50:15.687382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_random_image (class_names, images, labels):\n    index = np.random.randint(images.shape[0])\n    plt.figure()\n    plt.imshow(images[index])\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.title('Image #{}: '.format(index) + class_names[labels[index]])\n    plt.show()\n    \ndisplay_random_image (class_names, train_images, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:17.253157Z","iopub.execute_input":"2022-11-22T09:50:17.253498Z","iopub.status.idle":"2022-11-22T09:50:17.341888Z","shell.execute_reply.started":"2022-11-22T09:50:17.253467Z","shell.execute_reply":"2022-11-22T09:50:17.340903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def display_examples(class_names, images, labels):\n    fig = plt.figure(figsize = (10,10))\n    fig.suptitle(\"Examples of images in the dataset\", fontsize=16)\n    for i in range(25):\n        plt.subplot(5,5,i+1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.grid(False)\n        plt.imshow(images[i], cmap=plt.cm.binary)\n        plt.xlabel(class_names[labels[i]])\n    plt.show()\n    \ndisplay_examples(class_names, train_images, train_labels)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:19.178549Z","iopub.execute_input":"2022-11-22T09:50:19.178881Z","iopub.status.idle":"2022-11-22T09:50:20.259315Z","shell.execute_reply.started":"2022-11-22T09:50:19.178848Z","shell.execute_reply":"2022-11-22T09:50:20.258020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model build and training ","metadata":{}},{"cell_type":"code","source":"### Model number 2: Fine tuned Densenet201 model \n\n# Copy Densenet201 model and remove fully connected layers \n\ndensenet_model = VGG16(\n    include_top = False,\n    weights = 'imagenet', \n    input_tensor = None, \n    input_shape = (150,150,3), \n    pooling = None\n)\n\n# Option here to train on the final Conv5 block in the densenet model (\"True\") or use imagenet weights for this layer (\"False\")\n\ndensenet_model.trainable = True\n\nfor layer in densenet_model.layers:\n  if 'conv5' in layer.name:\n    layer.trainable = False\n  else:\n    layer.trainable = False\n\n# 3 fully connected layers are added, with 256, 128 and 64 units respectively\n# Dropout and Batch Normalization are performed on the fully connected layers.\n\ninput = Input(shape = (150,150,3))\nlayer = densenet_model(inputs=input)\nlayer = Flatten()(layer)\nlayer = BatchNormalization()(layer)\nlayer = Dense(units=256, activation='relu')(layer)\nlayer = Dropout(0.7)(layer)\nlayer = BatchNormalization()(layer)\nlayer = Dense(units=128, activation='relu')(layer)\nlayer = Dropout(0.5)(layer)\nlayer = Dense(units=64, activation='relu')(layer)\nlayer = Dropout(0.3)(layer)\nlayer = Dense(units=4,activation='softmax')(layer)\n\nmodel2 = Model(inputs=input, outputs=layer)\n\nmodel2.summary()\n\nmodel2.compile(loss='sparse_categorical_crossentropy',\n            optimizer= 'adam',\n            metrics=['accuracy'])\n\n# Implement callbacks \ncheckpoint = ModelCheckpoint(filepath='best_model.hdf5', save_best_only=True, save_weights_only=False)\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=3, verbose = 1, mode='min', restore_best_weights = True)\nlearning_rate_reduction = ReduceLROnPlateau(\n    monitor = 'val_accuracy', \n    patience = 2, \n    verbose = 1, \n    factor = 0.3, \n    min_lr = 0.000001)\n","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:23.338949Z","iopub.execute_input":"2022-11-22T09:50:23.339304Z","iopub.status.idle":"2022-11-22T09:50:26.312093Z","shell.execute_reply.started":"2022-11-22T09:50:23.339272Z","shell.execute_reply":"2022-11-22T09:50:26.311131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train model \n\nhistory2 = model2.fit(\n    train_images, \n    train_labels, \n    batch_size = 32, \n    epochs = 25, \n    validation_data =(val_images, val_labels) , \n    callbacks=[learning_rate_reduction])","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:50:30.627570Z","iopub.execute_input":"2022-11-22T09:50:30.627908Z","iopub.status.idle":"2022-11-22T09:55:51.487877Z","shell.execute_reply.started":"2022-11-22T09:50:30.627875Z","shell.execute_reply":"2022-11-22T09:55:51.486783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"# Create Accuracy and Loss chart function\n\ndef plot_accuracy_loss_chart(history):\n    epochs = [i for i in range(25)]\n    fig , ax = plt.subplots(1,2)\n    train_acc = history.history['accuracy']\n    train_loss = history.history['loss']\n    val_acc = history.history['val_accuracy']\n    val_loss = history.history['val_loss']\n    fig.set_size_inches(20,10)\n    ax[0].plot(epochs , train_acc , 'go-' , label = 'Training Accuracy')\n    ax[0].plot(epochs , val_acc , 'ro-' , label = 'Validation Accuracy')\n    ax[0].set_title('Training & Validation Accuracy')\n    ax[0].legend()\n    ax[0].set_xlabel(\"Epochs\")\n    ax[0].set_ylabel(\"Accuracy\")\n\n    ax[1].plot(epochs , train_loss , 'g-o' , label = 'Training Loss')\n    ax[1].plot(epochs , val_loss , 'r-o' , label = 'Validation Loss')\n    ax[1].set_title('Training & Validation Loss')\n    ax[1].legend()\n    ax[1].set_xlabel(\"Epochs\")\n    ax[1].set_ylabel(\"Training & Validation Loss\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:56:06.995325Z","iopub.execute_input":"2022-11-22T09:56:06.995712Z","iopub.status.idle":"2022-11-22T09:56:07.005002Z","shell.execute_reply.started":"2022-11-22T09:56:06.995675Z","shell.execute_reply":"2022-11-22T09:56:07.004042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Accuracy and Loss charts for Model 2\n\nplot_accuracy_loss_chart(history2)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:56:11.160167Z","iopub.execute_input":"2022-11-22T09:56:11.160517Z","iopub.status.idle":"2022-11-22T09:56:11.463289Z","shell.execute_reply.started":"2022-11-22T09:56:11.160482Z","shell.execute_reply":"2022-11-22T09:56:11.462187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save model \n\nmodel2.save(\"bloodcell_densenet201_10epochs_lrreduction.h5\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:56:15.526796Z","iopub.execute_input":"2022-11-22T09:56:15.527175Z","iopub.status.idle":"2022-11-22T09:56:15.782293Z","shell.execute_reply.started":"2022-11-22T09:56:15.527140Z","shell.execute_reply":"2022-11-22T09:56:15.781280Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluating the model on the test data\n\nresults = model2.evaluate(test_images, test_labels)\n\nprint(\"Loss of the model is - \", results[0])\nprint(\"Accuracy of the model is - \", results[1]*100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:56:17.241371Z","iopub.execute_input":"2022-11-22T09:56:17.241709Z","iopub.status.idle":"2022-11-22T09:56:19.313448Z","shell.execute_reply.started":"2022-11-22T09:56:17.241676Z","shell.execute_reply":"2022-11-22T09:56:19.312611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I was initially puzzled that Validation accuracy exceeded Training accuracy, and that Test accuracy exceeded both of these. I believe this is due to the Dropout layers which randomly set a proportion of unit inputs to 0, thus creating a more robust model that can generalise better to new data in the validation and test sets. ","metadata":{}},{"cell_type":"code","source":"# Create predictions for test images\n\npredictions2 = model2.predict(test_images)\npredictions2 = np.argmax(predictions2,axis=1)\npredictions2[:15]","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:56:38.883254Z","iopub.execute_input":"2022-11-22T09:56:38.883705Z","iopub.status.idle":"2022-11-22T09:56:41.143898Z","shell.execute_reply.started":"2022-11-22T09:56:38.883659Z","shell.execute_reply":"2022-11-22T09:56:41.142906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(\n    test_labels, \n    predictions2, \n    target_names = ['EOSINOPHIL (Class 0)', 'LYMPHOCYTE (Class 1)', 'MONOCYTE (Class 2)', 'NEUTROPHIL (Class 3)']))","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:57:18.152639Z","iopub.execute_input":"2022-11-22T09:57:18.153019Z","iopub.status.idle":"2022-11-22T09:57:18.167822Z","shell.execute_reply.started":"2022-11-22T09:57:18.152982Z","shell.execute_reply":"2022-11-22T09:57:18.167017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confusion matrix plot function and execute \n\ndef plot_confusion_matrix (cm):\n    plt.figure(figsize = (10,10))\n    sns.heatmap(\n        cm, \n        cmap = 'Blues', \n        linecolor = 'black', \n        linewidth = 1, \n        annot = True, \n        fmt = '', \n        xticklabels = class_names, \n        yticklabels = class_names)\n    \n","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:57:22.443538Z","iopub.execute_input":"2022-11-22T09:57:22.443986Z","iopub.status.idle":"2022-11-22T09:57:22.449833Z","shell.execute_reply.started":"2022-11-22T09:57:22.443913Z","shell.execute_reply":"2022-11-22T09:57:22.448943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot confusion matrix for Model 2\n\ncm2 = confusion_matrix(test_labels, predictions2)\ncm2 = pd.DataFrame(cm2, index = ['0', '1', '2', '3'], columns = ['0', '1', '2', '3'])\nplot_confusion_matrix(cm2)","metadata":{"execution":{"iopub.status.busy":"2022-11-22T09:57:24.578381Z","iopub.execute_input":"2022-11-22T09:57:24.578740Z","iopub.status.idle":"2022-11-22T09:57:24.805350Z","shell.execute_reply.started":"2022-11-22T09:57:24.578707Z","shell.execute_reply":"2022-11-22T09:57:24.804033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This Densenet Model shows similar patterns in test results that we saw in Model 1 (created in [Part 1](https://www.kaggle.com/kbrans/cnn-91-6-acc-with-new-train-val-test-splits)). Classification of Lymphocytes and Monocytes are very good with an F1 scores of 0.97 although not as good as in Model 1 (0.98 F1 score), while Neutrophils and Eosinophils and still confused on occasion with each other but fare slightly better than in Model 1. ","metadata":{}},{"cell_type":"markdown","source":"# Future Work \n\n- Create a Finetuned VGG16 model with the same dataset\n- Create a Densenet201 model where training occurs on final Conv5 block. \n- Explore optimization techniques and explore changes to hyperparameters (Learning rate, batch size, number of epochs etcs) ","metadata":{}},{"cell_type":"markdown","source":"# References \n\nSome code was adapted from [Carlos Zuluaga](https://medium.com/@carlosz22/transfer-learning-using-keras-with-densenet-169-91679300f94a), [Vincee](https://www.kaggle.com/vincee/intel-image-classification-cnn-keras) and [Abhinav Sagar](https://towardsdatascience.com/deep-learning-for-detecting-pneumonia-from-x-ray-images-fc9a3d9fdba8)","metadata":{}}]}